{"cells": [{"id": "952e87eb", "cell_type": "markdown", "source": "# \ud83e\udd16 Abschlussprojekt: RAG-System mit AI Index 2025", "metadata": {}}, {"id": "461dcb04", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udce6 Gerekli k\u00fct\u00fcphaneler\n!pip install pymupdf langchain tiktoken chromadb sentence-transformers --quiet", "outputs": []}, {"id": "10735991", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udcc1 PDF Y\u00fckle\nfrom google.colab import files\nuploaded = files.upload()", "outputs": []}, {"id": "21c14a3a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udcc4 Metin \u00e7\u0131karma\nimport fitz  # PyMuPDF\ndef extract_text_from_pdf(pdf_path):\n    doc = fitz.open(pdf_path)\n    return \"\".join([page.get_text() for page in doc])\npdf_path = 'hai_ai_index_report_2025.pdf'\nraw_text = extract_text_from_pdf(pdf_path)\nprint(f\"Metin uzunlu\u011fu: {len(raw_text)} karakter\")", "outputs": []}, {"id": "a3fa3db1", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83e\udde9 Metni chunk'lara b\u00f6l\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = text_splitter.split_text(raw_text)\nprint(f\"Toplam {len(chunks)} chunk \u00fcretildi.\")", "outputs": []}, {"id": "03c70f56", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83c\udff7\ufe0f Metadata ekle\nfrom langchain.docstore.document import Document\ndocuments = []\nfor i, chunk in enumerate(chunks):\n    metadata = {\"source\": \"AI Index 2025\", \"chunk_id\": i, \"section\": \"Investments\" if i < 50 else \"Other\"}\n    documents.append(Document(page_content=chunk, metadata=metadata))", "outputs": []}, {"id": "8374f3c4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udcbe ChromaDB ile indexleme\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\ndb = Chroma.from_documents(documents, embedding=embedding_function, persist_directory=\"rag_index_metadata\")\ndb.persist()", "outputs": []}, {"id": "6a5bbea2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udd0d Sade retrieval \u00f6rne\u011fi\nquery = \"What are the main AI investment trends in 2024?\"\nretrieved_docs = db.similarity_search(query, k=3)\nfor i, doc in enumerate(retrieved_docs, 1):\n    print(f\"--- Chunk {i} ---\\n{doc.page_content[:500]}\\n\")", "outputs": []}, {"id": "cd305814", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83c\udfaf Metadata filtreleme ile retrieval\nfiltered_docs = db.similarity_search(query, k=3, filter={\"section\": \"Investments\"})\nfor i, doc in enumerate(filtered_docs, 1):\n    print(f\"--- Filtered Chunk {i} (Section: {doc.metadata['section']}) ---\\n{doc.page_content[:500]}\\n\")", "outputs": []}, {"id": "61df1680", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udd01 Multi-query retrieval\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain.chat_models import ChatOpenAI\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nmulti_retriever = MultiQueryRetriever.from_llm(retriever=db.as_retriever(), llm=llm)\ndocs = multi_retriever.get_relevant_documents(query)\nfor i, doc in enumerate(docs[:3], 1):\n    print(f\"--- MultiQuery {i} ---\\n{doc.page_content[:500]}\\n\")", "outputs": []}, {"id": "88e36ad8", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83d\udcca LangSmith (izleme i\u00e7in, opsiyonel)\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-key\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"", "outputs": []}, {"id": "71fb5c92", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83e\udd16 LLM + RAG ile cevap \u00fcret\nfrom langchain.chains import RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(), chain_type=\"stuff\")\nresponse = qa_chain.run(query)\nprint(\"\\n\ud83e\udd16 Model Cevab\u0131:\\n\", response)", "outputs": []}, {"id": "46b0ef25", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \ud83e\uddea Evaluation: Retrieval a\u00e7\u0131k vs kapal\u0131\ntest_questions = [\n    \"What was the trend in global AI private investment in 2024?\",\n    \"Which countries led in AI research output in 2024?\",\n    \"How did AI adoption in education evolve in 2024?\",\n    \"What ethical concerns about AI are mentioned in the 2025 report?\",\n    \"Which sectors saw the highest AI implementation growth in 2024?\"\n]\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n=== Q{i}: {question} ===\")\n    print(\"\\n\ud83d\udeab LLM (no retrieval):\")\n    print(llm.predict(question))\n    print(\"\\n\u2705 RAG:\")\n    print(qa_chain.run(question))", "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}